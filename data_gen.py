import logging
import sqlite3
from dataclasses import dataclass
from textwrap import dedent
from typing import Iterator, List, Dict, Any, Literal

from diskcache import Cache
import litellm
from litellm import acompletion
from litellm.caching.caching import LiteLLMCacheType, Cache
from local_db import DB_PATH
from data_types import Function
from pydantic import BaseModel, Field
from rich import print

@dataclass
class FunctionSnippet():
    func_name: str
    func_documentation_string: str
    code_string: str

def iterate_repo_functions(
    repo_name: str,
    *,
    batch_size: int = 20,
    db_path: str = DB_PATH,
) -> Iterator[List[FunctionSnippet]]:
    """Yield batches of functions for the given repository.

    The function returns an *iterator* where each item is a list (batch) of
    FunctionSnippet objects whose size is at most ``batch_size``.

    Parameters
    ----------
    repo_name:
        The repository name (case-sensitive) whose functions we want to
        iterate over.
    batch_size:
        Size of the batch to yield on every iteration. Defaults to ``30``.
    db_path:
        Path to the SQLite database. Defaults to the database generated by
        ``local_db``.
    """
    
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row  # access columns by name

    # First, gather *all* function IDs that belong to this repository
    base_query_ids = """
        SELECT id
        FROM github_code
        WHERE repository_name = ?
        ORDER BY id
        """

    cursor = conn.execute(base_query_ids, (repo_name,))
    all_function_ids = [row["id"] for row in cursor.fetchall()]

    total = len(all_function_ids)
    if total == 0:
        conn.close()
        return  # Empty iterator

    # Helper to fetch a batch of FunctionSnippet objects given a slice of ids
    def _fetch_batch(id_slice: List[int]) -> List[FunctionSnippet]:
        placeholders = ",".join(["?"] * len(id_slice))
        functions_query = f"""
            SELECT func_name, func_documentation_string, func_code_tokens
            FROM github_code
            WHERE id IN ({placeholders})
            ORDER BY id
        """
        function_rows = conn.execute(functions_query, id_slice).fetchall()

        batch: List[FunctionSnippet] = []
        for row in function_rows:
            batch.append(FunctionSnippet(
                func_name=row["func_name"],
                func_documentation_string=row["func_documentation_string"],
                code_string=row["func_code_tokens"],
            ))
        return batch

    # Yield batches lazily
    for start in range(0, total, batch_size):
        id_batch = all_function_ids[start : start + batch_size]
        yield _fetch_batch(id_batch)

    conn.close()



litellm.cache = Cache(type=LiteLLMCacheType.DISK)


class GeneratedSyntheticQuery(BaseModel):
    question: str
    answer: str
    repo: str
    functions: List[str]
    how_realistic: float = Field(
        ...,
        description="Give a score between 0 and 1 on how realistic this question is. That is, how likely is it that the user would actually ask this about the repo?",
    )

class Response(BaseModel):
    questions: List[GeneratedSyntheticQuery]
    
async def generate_synthetic_qa_pairs_for_repo(repo: str, batch: List[FunctionSnippet]) -> Response:
    """
    Generate synthetic QA pairs for a given repository.
    """
    
    SYSTEM_PROMPT = dedent(
        f"""
        You are an assistant that creates realistic question–answer pairs a human might ask about github functions in a repo.
        For example, how a user might use the functions in the repo to complete a task.
        Every answer MUST be fully contained in the provided functions. Do NOT hallucinate.

        Do not include functions in the question.

        Respond with a JSON object with the following structure:
        {Response.model_json_schema()}
        """
    ).strip()

    user_query = dedent(
        f"""
        Here are {len(batch)} functions from {repo}:
        ---
        {batch}
        ---

        Generate 8 diverse question–answer pairs for the batch of issues.
        """
    ).strip()
    
    resp = await acompletion(
    model="gpt-4.1",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_query},
        ],
        caching=True,
        stream=False,
        response_format=Response,
    )

    content = resp["choices"][0]["message"]["content"]  # type: ignore
    qa_pairs = Response.model_validate_json(content).questions

    return qa_pairs

def filter_repos(db_path: str, split_type: Literal["train", "test"], min_count: int = 10):
    """
    Return a nested dictionary mapping each repository_name to a dict of split_name to count,
    filtered by minimum count threshold.
    Args:
        db_path: Path to the SQLite database
        split_type: Either "train" to get train split data, or "test" to get test and valid split data
        min_count: Minimum number of functions required for a repository to be included
    Returns:
        dict: {repository_name: {split_name: count}} where count >= min_count
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    if split_type == "train":
        split_filter = "WHERE split_name = 'train'"
    elif split_type == "test":
        split_filter = "WHERE split_name IN ('test', 'valid')"
    
    cursor.execute(f"""
        SELECT repository_name, split_name, COUNT(*)
        FROM github_code
        {split_filter}
        GROUP BY repository_name, split_name
        HAVING COUNT(*) >= ?
    """, (min_count,))
    
    result = {}
    for repo, split, count in cursor.fetchall():
        if repo not in result:
            result[repo] = {}
        result[repo][split] = count
    conn.close()
    logging.info(f"Repository counts by split ({split_type}): {result}")
    return result

    
async def generate_synthetic_data_for_repo(repo: str, batch_size: int = 20) -> List[Response]:
    """
    Generate synthetic data for a given repository.
    """
    for batch in iterate_repo_functions(repo, batch_size=batch_size):
        qa_pairs = await generate_synthetic_qa_pairs_for_repo(repo, batch)
        for qa_pair in qa_pairs:
            print(qa_pair.question)
            print(qa_pair.answer)
            print(qa_pair.repo)
            print(qa_pair.functions)
            print(qa_pair.how_realistic)
            print("-" * 100)
        
    

if __name__ == "__main__":
    import asyncio
    # asyncio.run(generate_synthetic_data_for_repo("deepmind/sonnet")) 
    print(filter_repos(DB_PATH, "test", 100))