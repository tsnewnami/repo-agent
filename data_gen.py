import logging
import sqlite3
from dataclasses import dataclass
from textwrap import dedent
from typing import Iterator, List, Dict, Any, Literal

from diskcache import Cache
import litellm
from litellm import acompletion
from litellm.caching.caching import LiteLLMCacheType, Cache
from local_db import DB_PATH
from data_types import Function
from pydantic import BaseModel, Field
from rich import print

@dataclass
class FunctionSnippet():
    name: str
    documentation_string: str
    code_tokens: str
    path: str

def iterate_repo_functions(
    repo_name: str,
    *,
    batch_size: int = 20,
    db_path: str = DB_PATH,
) -> Iterator[List[FunctionSnippet]]:
    """Yield batches of functions for the given repository.

    The function returns an *iterator* where each item is a list (batch) of
    FunctionSnippet objects whose size is at most ``batch_size``.

    Parameters
    ----------
    repo_name:
        The repository name (case-sensitive) whose functions we want to
        iterate over.
    batch_size:
        Size of the batch to yield on every iteration. Defaults to ``30``.
    db_path:
        Path to the SQLite database. Defaults to the database generated by
        ``local_db``.
    """
    
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row  # access columns by name

    # First, gather *all* function IDs that belong to this repository
    base_query_ids = """
        SELECT id
        FROM github_code
        WHERE repository_name = ?
        ORDER BY id
        """

    cursor = conn.execute(base_query_ids, (repo_name,))
    all_function_ids = [row["id"] for row in cursor.fetchall()]

    total = len(all_function_ids)
    if total == 0:
        conn.close()
        return  # Empty iterator

    # Helper to fetch a batch of FunctionSnippet objects given a slice of ids
    def _fetch_batch(id_slice: List[int]) -> List[FunctionSnippet]:
        placeholders = ",".join(["?"] * len(id_slice))
        functions_query = f"""
            SELECT func_name, func_documentation_string, func_code_tokens, func_path_in_repository
            FROM github_code
            WHERE id IN ({placeholders})
            ORDER BY id
        """
        function_rows = conn.execute(functions_query, id_slice).fetchall()

        batch: List[FunctionSnippet] = []
        for row in function_rows:
            batch.append(FunctionSnippet(
                name=row["func_name"],
                documentation_string=row["func_documentation_string"],
                code_tokens=row["func_code_tokens"],
                path=row["func_path_in_repository"],
            ))
        return batch

    # Yield batches lazily
    for start in range(0, total, batch_size):
        id_batch = all_function_ids[start : start + batch_size]
        yield _fetch_batch(id_batch)

    conn.close()



litellm.cache = Cache(type=LiteLLMCacheType.DISK)


class GeneratedSyntheticQuery(BaseModel):
    question: str
    answer: str
    repo: str
    functions: List[str]
    how_realistic: float = Field(
        ...,
        description="Give a score between 0 and 1 on how realistic this question is. That is, how likely is it that the user would actually ask this about the repo?",
    )

class Response(BaseModel):
    questions: List[GeneratedSyntheticQuery]
    
async def generate_synthetic_qa_pairs_for_repo(repo: str, batch: List[FunctionSnippet]) -> Response:
    """
    Generate synthetic QA pairs for a given repository.
    """
    
    SYSTEM_PROMPT = dedent(
        f"""
        You are an assistant that creates realistic question–answer pairs that developers would actually ask about GitHub functions in a repository.

        ## Core Principles:
        1. **Realistic & Task-Oriented**: Generate questions that reflect real developer needs - "How do I..." rather than "What does..."
        2. **Contextually Appropriate**: Use function paths, names, and documentation to understand relationships and generate appropriate question types
        3. **Fully Grounded**: Every answer MUST be completely answerable from the provided functions. Do NOT hallucinate functionality.
        4. **Diverse Question Types**: Mix individual function questions with multi-function workflow questions based on what makes sense for the batch

        ## Question Generation Strategy:

        ### Analyze the Batch First:
        - **File Path Context**: Functions in the same file/directory are likely related and suitable for multi-function questions
        - **Domain Recognition**: Use paths to understand the domain (auth/, database/, utils/, api/, etc.) and craft domain-appropriate questions
        - **Function Relationships**: Look for functions that naturally work together (complementary operations, shared parameters, sequential workflows)

        ### Question Types to Generate:

        **Task-Oriented Questions (60-70% of questions):**
        - "How do I authenticate users in this system?"
        - "How do I process and validate user input?"
        - "How do I set up error handling for API calls?"

        **Function Discovery Questions:**
        - "What should I use to hash passwords securely?"
        - "Which function handles file uploads?"
        - "What's available for database connection management?"

        **Usage Pattern Questions (when functions are related):**
        - "What's the typical workflow for processing requests?"
        - "How do these validation functions work together?"
        - "What's the correct sequence for initialization?"

        **Integration & Configuration Questions:**
        - "How do I configure retry settings for this client?"
        - "What parameters are required for this database connection?"
        - "How do I handle errors when this function fails?"

        ### Adaptive Question Generation:
        - **Same file/directory**: Favor multi-function questions about workflows and integration
        - **Different domains**: Generate individual function questions focused on specific use cases
        - **Utility functions**: Focus on practical usage and common scenarios
        - **Complex functions**: Include error handling and edge cases in questions

        ### Answer Requirements:
        - Provide concrete, actionable guidance
        - Include code examples when helpful
        - Address common pitfalls and best practices
        - Reference specific function parameters and return values
        - Explain error conditions and handling

        ## Quality Markers:
        - Questions should sound like they come from Stack Overflow or developer forums
        - Answers should be immediately actionable by a developer
        - Focus on practical implementation over theoretical understanding
        - Consider different skill levels (beginner, intermediate, advanced)

        **Important**: Do not mention specific function names in questions. Ask about capabilities and tasks instead.

        Respond with a JSON object with the following structure:
        {Response.model_json_schema()}
        """
    ).strip()

    user_query = dedent(
        f"""
        Here are {len(batch)} functions from {repo}:
        ---
        {batch}
        ---

        Generate 8 diverse question–answer pairs for the batch of functions.
        """
    ).strip()
    
    resp = await acompletion(
    model="gpt-4.1",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_query},
        ],
        caching=True,
        stream=False,
        response_format=Response,
    )

    content = resp["choices"][0]["message"]["content"]  # type: ignore
    qa_pairs = Response.model_validate_json(content).questions

    return qa_pairs

def filter_repos(db_path: str, split_type: Literal["train", "test"], min_count: int = 50):
    """
    Return a nested dictionary mapping each repository_name to a dict of split_name to count,
    filtered by minimum count threshold.
    Args:
        db_path: Path to the SQLite database
        split_type: Either "train" to get train split data, or "test" to get test and valid split data
        min_count: Minimum number of functions required for a repository to be included
    Returns:
        dict: {repository_name: {split_name: count}} where count >= min_count
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    if split_type == "train":
        split_filter = "WHERE split_name = 'train'"
    elif split_type == "test":
        split_filter = "WHERE split_name IN ('test', 'valid')"
    
    cursor.execute(f"""
        SELECT repository_name, split_name, COUNT(*)
        FROM github_code
        {split_filter}
        GROUP BY repository_name, split_name
        HAVING COUNT(*) >= ?
    """, (min_count,))
    
    result = {}
    for repo, split, count in cursor.fetchall():
        if repo not in result:
            result[repo] = {}
        result[repo][split] = count
    conn.close()
    logging.info(f"Repository counts by split ({split_type}): {result}")
    return result

    
async def generate_synthetic_data_for_repo(repo: str, batch_size: int = 20) -> List[Response]:
    """
    Generate synthetic data for a given repository.
    """
    for batch in iterate_repo_functions(repo, batch_size=batch_size):
        qa_pairs = await generate_synthetic_qa_pairs_for_repo(repo, batch)
        for qa_pair in qa_pairs:
            print(qa_pair.question)
            print(qa_pair.answer)
            print(qa_pair.repo)
            print(qa_pair.functions)
            print(qa_pair.how_realistic)
            print("-" * 100)
        
    

if __name__ == "__main__":
    import asyncio
    asyncio.run(generate_synthetic_data_for_repo("deepmind/sonnet")) 
    # for batch in iterate_repo_functions("deepmind/sonnet", batch_size=10):
    #     print(batch)
    #     break
    # print(filter_repos(DB_PATH, "test", 100))