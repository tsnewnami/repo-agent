import logging
import sqlite3
import json
from dataclasses import dataclass
from textwrap import dedent
import time
from typing import Iterator, List, Dict, Any, Literal
from pathlib import Path

from diskcache import Cache
import litellm
from litellm import acompletion
from litellm.caching.caching import LiteLLMCacheType, Cache
from local_db import DB_PATH
from pydantic import BaseModel, Field
from rich import print
from tqdm import tqdm

@dataclass
class FunctionSnippet():
    name: str
    documentation_string: str
    code_tokens: str
    path: str

def iterate_repo_functions(
    repo_name: str,
    *,
    batch_size: int,
    db_path: str = DB_PATH,
) -> Iterator[List[FunctionSnippet]]:
    """Yield batches of functions for the given repository.

    The function returns an *iterator* where each item is a list (batch) of
    FunctionSnippet objects whose size is at most ``batch_size``.

    Parameters
    ----------
    repo_name:
        The repository name (case-sensitive) whose functions we want to
        iterate over.
    batch_size:
        Size of the batch to yield on every iteration. Defaults to ``30``.
    db_path:
        Path to the SQLite database. Defaults to the database generated by
        ``local_db``.
    """
    
    def _trim_string(text: str, max_length: int) -> str:
        """Trim a string if it exceeds max_length while preserving readability."""
        if not text or len(text) <= max_length:
            return text
        # Keep the first 2/3 and last 1/3 of the allowed length
        first_part = int(max_length * 2/3)
        last_part = max_length - first_part - 5  # 5 chars for ellipsis and spacing
        return f"{text[:first_part]} ... {text[-last_part:]}"
    
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row  # access columns by name

    # First, gather *all* function IDs that belong to this repository
    base_query_ids = """
        SELECT id
        FROM github_code
        WHERE repository_name = ?
        ORDER BY id
        """

    cursor = conn.execute(base_query_ids, (repo_name,))
    all_function_ids = [row["id"] for row in cursor.fetchall()]

    total = len(all_function_ids)
    if total == 0:
        conn.close()
        return  # Empty iterator

    # Helper to fetch a batch of FunctionSnippet objects given a slice of ids
    def _fetch_batch(id_slice: List[int]) -> List[FunctionSnippet]:
        placeholders = ",".join(["?"] * len(id_slice))
        functions_query = f"""
            SELECT func_name, func_documentation_string, func_code_tokens, func_path_in_repository
            FROM github_code
            WHERE id IN ({placeholders})
            ORDER BY id
        """
        function_rows = conn.execute(functions_query, id_slice).fetchall()

        batch: List[FunctionSnippet] = []
        for row in function_rows:
            # Trim long strings to avoid context window issues
            doc_string = _trim_string(row["func_documentation_string"], 500)
            code_tokens = _trim_string(row["func_code_tokens"], 500)
            
            batch.append(FunctionSnippet(
                name=row["func_name"],
                documentation_string=doc_string,
                code_tokens=code_tokens,
                path=row["func_path_in_repository"],
            ))
        return batch

    # Yield batches lazily
    for start in range(0, total, batch_size):
        id_batch = all_function_ids[start : start + batch_size]
        yield _fetch_batch(id_batch)

    conn.close()



litellm.cache = Cache(type=LiteLLMCacheType.DISK)


class GeneratedSyntheticQuery(BaseModel):
    question: str
    answer: str
    repo: str
    functions: List[str]
    how_realistic: float = Field(
        ...,
        description="Give a score between 0 and 1 on how realistic this question is. That is, how likely is it that the user would actually ask this about the repo?",
    )

class Response(BaseModel):
    questions: List[GeneratedSyntheticQuery]
    
async def generate_synthetic_qa_pairs_for_repo(repo: str, batch: List[FunctionSnippet]) -> Response:
    """
    Generate synthetic QA pairs for a given repository.
    """
    
    SYSTEM_PROMPT = dedent(
    f"""
        We are training a code assistant that helps developers understand and use functions in a repository. The user will ask questions about code functionality, and the assistant will find relevant functions and provide actionable answers.

        Your job is to generate synthetic training data for the assistant. You will be passed a batch of functions from the repository, and you need to generate plausible questions that a developer might ask, whose answers are all contained in the provided functions. The questions should be practical and actionable, with concrete answers present in the function code and documentation.

        For each question, you should return the correct answer as well as the function names that contain the answer. Note that some batches of functions might not be good candidates for generating training data. In that case you can return an empty list.

        - Questions should be from a developer's perspective working with this codebase. Eg. 'How do I validate user input?' or 'What's the best way to handle authentication errors?'
        - Mix simple questions (answerable from 1-2 functions) with complex questions (requiring 3+ functions) to train adaptive retrieval
        - Focus on practical implementation questions that developers actually face
        - Avoid mentioning specific function names in questions - ask about capabilities instead
        - Return only a JSON list of objects. Each object should have the following fields:
            - question: string, (The question a developer might ask)
            - answer: string, (The actionable answer with code guidance)
            - function_names: string[], (The function names that contain the answer)
            - how_realistic: float, (How likely a developer would actually ask this, between 0 and 1)

        Generate questions that train the agent to use appropriate retrieval strategies while remaining practically useful to developers.
        {Response.model_json_schema()}
    """
    ).strip()

    user_query = dedent(
        f"""
        Here are {len(batch)} functions from {repo}:
        ---
        {batch}
        ---

        Generate 4 diverse questionâ€“answer pairs for the batch of functions.
        """
    ).strip()
    
    resp = await acompletion(
        model="gpt-4.1",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_query},
        ],
        caching=True,
        stream=False,
        response_format=Response,
    )

    content = resp["choices"][0]["message"]["content"]  # type: ignore
    qa_pairs = Response.model_validate_json(content).questions

    return qa_pairs

def filter_repos(db_path: str, split_type: Literal["train", "test"], min_func_count: int = 50) -> List[str]:
    """
    Return a list of repository names filtered by minimum function count threshold.
    
    Args:
        db_path: Path to the SQLite database
        split_type: Either "train" to get train split data, or "test" to get test and valid split data
        min_func_count: Minimum number of functions required for a repository to be included
    Returns:
        List[str]: List of repository names where function count >= min_func_count
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    if split_type == "train":
        split_filter = "WHERE split_name = 'train'"
    elif split_type == "test":
        split_filter = "WHERE split_name IN ('test', 'valid')"
    
    cursor.execute(f"""
        SELECT repository_name
        FROM github_code
        {split_filter}
        GROUP BY repository_name
        HAVING COUNT(*) >= ?
    """, (min_func_count,))
    
    repos = [row[0] for row in cursor.fetchall()]
    conn.close()
    
    logging.info(f"Found {len(repos)} repositories for {split_type} split: {repos}")
    return repos

    
async def generate_synthetic_data_for_repo(repo: str, batch_size: int) -> List[GeneratedSyntheticQuery]:
    """
    Generate synthetic data for a given repository.
    """
    all_qa_pairs = []
    # REMOVE
    temp_count = 0
    for batch in iterate_repo_functions(repo, batch_size=batch_size):
        qa_pairs = await generate_synthetic_qa_pairs_for_repo(repo, batch)
        all_qa_pairs.extend(qa_pairs)
        temp_count += 1
        if temp_count > 3:
            break
        
        # Log progress
        print(f"Generated {len(qa_pairs)} QA pairs for {repo} (batch of {len(batch)} functions)")
    
    return all_qa_pairs

async def generate_and_write_synthetic_data(
    output_dir: str,
    split_type: Literal["train", "test"],
    min_func_count: int = 50,
    batch_size: int = 50,
) -> None:
    """
    Generate synthetic data for repositories in the specified split and write to JSONL files.
    
    Args:
        split_type: Either "train" or "test" to determine which repositories to process
        min_count: Minimum number of functions required for a repository to be included
        batch_size: Number of functions to process in each batch
        output_dir: Directory to write the JSONL files to
    """
    # Create output directory if it doesn't exist
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Get repositories for this split type
    repos = filter_repos(DB_PATH, split_type, min_func_count)

    if not repos:
        print(f"No repositories found for split type: {split_type}")
        return
    
    # Process each repository
    total_repos = len(repos)
    repo_progress = tqdm(repos, 
                        desc=f"Processing {split_type} repositories", 
                        total=total_repos,
                        unit="repo")
    
    total_qa_pairs = 0
    processed_repos = 0
    
    for repo_name in repo_progress:
        repo_progress.set_postfix_str(f"Current: {repo_name}")
        
        try:
            # Generate synthetic data for this repository
            qa_pairs = await generate_synthetic_data_for_repo(repo_name, batch_size)
            
            if not qa_pairs:
                repo_progress.write(f"No QA pairs generated for {repo_name}")
                continue
            
            output_file = output_path / f"{split_type}.jsonl"
            
            with open(output_file, 'a', encoding='utf-8') as f:
                for qa_pair in qa_pairs:
                    # Create JSONL entry with split information
                    jsonl_entry = {
                        "question": qa_pair.question,
                        "answer": qa_pair.answer,
                        "repo": qa_pair.repo,
                        "functions": qa_pair.functions,
                        "how_realistic": qa_pair.how_realistic,
                        "split": split_type
                    }
                    f.write(json.dumps(jsonl_entry) + '\n')
            
            repo_progress.write(f"Wrote {len(qa_pairs)} QA pairs to {output_file}")
            
            total_qa_pairs += len(qa_pairs)
            processed_repos += 1
            
        except Exception as e:
            repo_progress.write(f"Error processing repository {repo_name}: {e}")
            continue
    
    repo_progress.close()
    print(f"\nCompleted {split_type} split:")
    print(f"  - Processed {processed_repos}/{total_repos} repositories")
    print(f"  - Generated {total_qa_pairs} total QA pairs")

async def main():
    output_dir = "synthetic_data"
    """
    Main function to generate synthetic data for both train and test splits.
    """
    print("Starting synthetic data generation...")
    
    # # Generate data for train split
    # print("\n=== Generating data for TRAIN split ===")
    # await generate_and_write_synthetic_data(output_dir, "train", min_func_count=50, batch_size=25)
    
    # Generate data for test split  
    print("\n=== Generating data for TEST split ===")
    await generate_and_write_synthetic_data(output_dir, "test", min_func_count=50, batch_size=25)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main()) 